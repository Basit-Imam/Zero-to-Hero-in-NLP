{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h4>Unit 2 <h1 style=\"text-align:center\"> Chapter 5</h1>\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random process is called <strong>stationary</strong> if the probabilities it assigns to a word or sequence is invariant with respect to shift in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In other words, the probability distribution for words at time $t$ is the same as the probability distribution at time $t+1$. Markov models, and hence n-grams, are stationary. For example, in a bigram, $P_i$ is dependent only on $P_{i−1}$. So if we shift our time index by $x$, $P_{i+x}$ is still dependent on $P_{i+x−1}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But natural languages are not stationary. We will be discussing about this in the coming chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy and Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose there are two random events,\n",
    "\n",
    "1. Diffusing a bomb by cutting one of the two wires, red or blue.\n",
    "2. Punch Mike Tyson.\n",
    "\n",
    "\n",
    "The possibilties of both the events is same. You either live or die.\n",
    "\n",
    "But the first event is more exciting than the second because we know what will happen when we punch Mike Tyson.\n",
    "\n",
    "In other words we can say that, we gain *less information* when we punch Mike Tyson(because we partially know).\n",
    "\n",
    "But we can't predict what would happen after the first event.\n",
    "\n",
    "Formally, diffusing a bomb has *more entropy* than punching Mike Tyson.\n",
    "\n",
    "### Entropy \n",
    "\n",
    "> Lack of predictability and order.\n",
    "\n",
    "As a result, \n",
    "\n",
    "Higher the entropy, more lack of predictibility.\n",
    "\n",
    "Higher the entropy, more information you gain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "* To learn more on this click [here](https://machinelearningmastery.com/what-is-information-entropy/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
